{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897a46eb-8bd3-43d2-a8bc-4a1df06864f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **1. Importing the required modules and functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4613a2b4-ee1a-42e7-9aba-bf9119978620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required modules and functions\n",
    "from pyspark.sql.functions import col, trim, upper, lower, initcap, translate, to_timestamp, to_date, current_timestamp, year, count, sum, round, row_number\n",
    "from pyspark.sql import Window\n",
    "from delta.tables import DeltaTable\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "529c9871-37a3-4424-a854-888f55a428b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **2. Abstract base class for factory interface.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b000c9-4efa-4cb2-be7a-1fc707741193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating abstract base class for factory interface\n",
    "class Silver(ABC):\n",
    "    # Basic representation of the data extraction, transformation & loading codes\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_bronze(self, spark, src_file_path, src_columns):\n",
    "        # Read and returns a pyspark dataframe from the parquet files in Bronze layer\n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        # Returns the transformed pyspark dataframe\n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "        # Performs upsert/merge for the Delta tables in Silver layer with the transformed pyspark dataframe \n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c6c15f-24e9-4507-9240-0b042311886e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **3. Concrete classes and implementing the abstract methods in subclasses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366b9a01-033c-48ed-9183-4bf63f043921",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Customers(Silver):\n",
    "\n",
    "    \"\"\" Method to read the parquet file from bronze staging layer with read options & return the pyspark dataframe     \n",
    "    \"\"\"\n",
    "    def read_bronze(self, spark, src_file_path, src_columns): \n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        bronze_df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{src_file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        bronze_df = (\n",
    "            bronze_df \\\n",
    "            .select(*src_columns)\n",
    "        )\n",
    "\n",
    "        return bronze_df\n",
    "    \n",
    "\n",
    "    # Method to perform transformations in the customers bronze source pyspark dataframe\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        \n",
    "        # DATA QUALITY CHECK\n",
    "        transformed_df = bronze_df.filter(\"customer_id IS NOT NULL\")        \n",
    "\n",
    "        # DATA DEDUPLICATION\n",
    "        customers_windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"customer_id\")\n",
    "\n",
    "        transformed_df = transformed_df.withColumn(\"row_number\", row_number().over(customers_windowSpec)) \\\n",
    "                .filter(\"row_number = 1\") \\\n",
    "                .drop(\"row_number\")\n",
    "                \n",
    "        # CORRECTING DATASTRUCTURE ISSUES\n",
    "        transformed_df = transformed_df.select(\n",
    "                        upper(trim(\"customer_id\")).alias(\"customer_id\"),\n",
    "                        lower(trim(\"customer_email_id\")).alias(\"customer_email_id\"),\n",
    "                        initcap(trim(\"customer_state\")).alias(\"customer_state\"),\n",
    "                        upper(trim(\"customer_state_code\")).alias(\"customer_state_code\")\n",
    "                        )\n",
    "        \n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    # Method to perform upsert/merge for customers delta table in silver layer\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "\n",
    "        # Reading customers delta table from silver layer\n",
    "        silver_olist_customers = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Performing upsert to the customers delta table in silver layer\n",
    "        silver_olist_customers.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            transformed_df.alias(\"src\"),\n",
    "            (\"tgt.customer_id = src.customer_id\")\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "            {\n",
    "            \"customer_email_id\": \"src.customer_email_id\",\n",
    "            \"customer_state\": \"src.customer_state\",\n",
    "            \"customer_state_code\": \"src.customer_state_code\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "            {\n",
    "            \"customer_id\": \"src.customer_id\",\n",
    "            \"customer_email_id\": \"src.customer_email_id\",\n",
    "            \"customer_state\": \"src.customer_state\",\n",
    "            \"customer_state_code\": \"src.customer_state_code\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d98a96-7396-46e8-a695-85f45d417d40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Orders(Silver):\n",
    "\n",
    "    \"\"\" Method to read the parquet file from bronze staging layer with read options & return the pyspark dataframe     \n",
    "    \"\"\"\n",
    "    def read_bronze(self, spark, src_file_path, src_columns): \n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        bronze_df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{src_file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        bronze_df = (\n",
    "            bronze_df \\\n",
    "            .select(*src_columns)\n",
    "        )\n",
    "\n",
    "        return bronze_df\n",
    "    \n",
    "\n",
    "    # Method to perform transformations in the orders bronze source pyspark dataframe\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        \n",
    "        # DATA QUALITY CHECK\n",
    "        transformed_df = bronze_df.filter(\"order_id is NOT NULL AND customer_id IS NOT NULL\")        \n",
    "\n",
    "        # DATA DEDUPLICATION\n",
    "        orders_windowSpec = Window.partitionBy(\"order_id\", \"customer_id\").orderBy(\"order_id\", \"customer_id\")\n",
    "\n",
    "        transformed_df = transformed_df.withColumn(\"row_number\", row_number().over(orders_windowSpec)) \\\n",
    "                .filter(\"row_number = 1\") \\\n",
    "                .drop(\"row_number\")\n",
    "                \n",
    "        # CORRECTING DATASTRUCTURE ISSUES\n",
    "        transformed_df = transformed_df.select(\n",
    "                        upper(trim(\"order_id\")).alias(\"order_id\"),\n",
    "                        upper(trim(\"customer_id\")).alias(\"customer_id\"),\n",
    "                        initcap(trim(\"order_status\")).alias(\"order_status\"),\n",
    "                        to_timestamp(\"order_purchase_timestamp\", timestamp_format).alias(\"order_purchase_timestamp\"),\n",
    "                        to_timestamp(\"order_approved_at\", timestamp_format).alias(\"order_approved_at\"),\n",
    "                        to_timestamp(\"order_delivered_carrier_date\", timestamp_format).alias(\"order_delivered_carrier_date\"),\n",
    "                        to_timestamp(\"order_delivered_customer_date\", timestamp_format).alias(\"order_delivered_customer_date\"),\n",
    "                        to_date(to_timestamp(\"order_estimated_delivery_date\", timestamp_format), \"yyyy-MM-dd\").alias(\"order_estimated_delivery_date\")                        \n",
    "                        )\n",
    "        \n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    # Method to perform upsert/merge for orders delta table in silver layer\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "\n",
    "        # Reading orders delta table from silver layer\n",
    "        silver_olist_orders = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Performing upsert to the orders delta table in silver layer\n",
    "        silver_olist_orders.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            transformed_df.alias(\"src\"),\n",
    "            (\"\"\"\n",
    "            ((tgt.order_id = src.order_id) and \n",
    "             (tgt.customer_id = src.customer_id))\n",
    "            \"\"\")\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "            {\n",
    "            \"order_status\": \"src.order_status\",\n",
    "            \"order_purchase_timestamp\": \"src.order_purchase_timestamp\",\n",
    "            \"order_approved_at\": \"src.order_approved_at\",\n",
    "            \"order_delivered_carrier_date\": \"src.order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\": \"src.order_delivered_customer_date\",        \n",
    "            \"order_estimated_delivery_date\": \"src.order_estimated_delivery_date\",                        \n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "            {      \n",
    "            \"order_id\": \"src.order_id\",\n",
    "            \"customer_id\": \"src.customer_id\",                      \n",
    "            \"order_status\": \"src.order_status\",\n",
    "            \"order_purchase_timestamp\": \"src.order_purchase_timestamp\",\n",
    "            \"order_approved_at\": \"src.order_approved_at\",\n",
    "            \"order_delivered_carrier_date\": \"src.order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\": \"src.order_delivered_customer_date\",        \n",
    "            \"order_estimated_delivery_date\": \"src.order_estimated_delivery_date\",                        \n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eed47c7-2234-448f-b5b0-d3c509bf03ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Products(Silver):\n",
    "\n",
    "    \"\"\" Method to read the parquet file from bronze staging layer with read options & return the pyspark dataframe     \n",
    "    \"\"\"\n",
    "    def read_bronze(self, spark, src_file_path, src_columns): \n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        bronze_df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{src_file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        bronze_df = (\n",
    "            bronze_df \\\n",
    "            .select(*src_columns)\n",
    "        )\n",
    "\n",
    "        return bronze_df\n",
    "    \n",
    "\n",
    "    # Method to perform transformations in the products bronze source pyspark dataframe\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        \n",
    "        # DATA QUALITY CHECK\n",
    "        transformed_df = bronze_df.filter(\"product_id is NOT NULL AND seller_id IS NOT NULL\")        \n",
    "\n",
    "        # DATA DEDUPLICATION\n",
    "        products_windowSpec = Window.partitionBy(\"product_id\", \"seller_id\").orderBy(\"product_id\", \"seller_id\")\n",
    "\n",
    "        transformed_df = transformed_df.withColumn(\"row_number\", row_number().over(products_windowSpec)) \\\n",
    "                .filter(\"row_number = 1\") \\\n",
    "                .drop(\"row_number\")\n",
    "                \n",
    "        # CORRECTING DATASTRUCTURE ISSUES\n",
    "        transformed_df = transformed_df.select(\n",
    "                        upper(trim(\"product_id\")).alias(\"product_id\"),\n",
    "                        upper(trim(\"seller_id\")).alias(\"seller_id\"),\n",
    "                        initcap(trim(\"product_category_name\")).alias(\"product_category_name\"),\n",
    "                        initcap(trim(\"product_category_name_english\")).alias(\"product_category_name_english\"),\n",
    "                        col(\"price\").cast(\"float\").alias(\"price\"),\n",
    "                        col(\"freight_value\").cast(\"float\").alias(\"freight_value\"),\n",
    "                        col(\"product_name_length\").cast(\"int\").alias(\"product_name_length\"),\n",
    "                        col(\"product_description_length\").cast(\"int\").alias(\"product_description_length\"),\n",
    "                        col(\"product_photos_qty\").cast(\"int\").alias(\"product_photos_qty\"),\n",
    "                        col(\"product_weight_g\").cast(\"float\").alias(\"product_weight_g\"),\n",
    "                        col(\"product_length_cm\").cast(\"float\").alias(\"product_length_cm\"),\n",
    "                        col(\"product_height_cm\").cast(\"float\").alias(\"product_height_cm\"),\n",
    "                        col(\"product_width_cm\").cast(\"float\").alias(\"product_width_cm\")\n",
    "                        )\n",
    "        \n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    # Method to perform upsert/merge for products delta table in silver layer\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "\n",
    "        # Reading products delta table from silver layer\n",
    "        silver_olist_products = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Performing upsert to the products delta table in silver layer\n",
    "        silver_olist_products.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            transformed_df.alias(\"src\"),\n",
    "            (\"\"\"\n",
    "            ((tgt.product_id = src.product_id) and \n",
    "             (tgt.seller_id = src.seller_id))\n",
    "            \"\"\")\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "            {\n",
    "            \"product_category_name\": \"src.product_category_name\",\n",
    "            \"product_category_name_english\": \"src.product_category_name_english\",\n",
    "            \"price\": \"src.price\",\n",
    "            \"freight_value\": \"src.freight_value\",\n",
    "            \"product_name_length\": \"src.product_name_length\",\n",
    "            \"product_description_length\": \"src.product_description_length\",\n",
    "            \"product_photos_qty\": \"src.product_photos_qty\",\n",
    "            \"product_weight_g\": \"src.product_weight_g\",\n",
    "            \"product_length_cm\": \"src.product_length_cm\",\n",
    "            \"product_height_cm\": \"src.product_height_cm\",\n",
    "            \"product_width_cm\": \"src.product_width_cm\",                                                \n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "            {\n",
    "            \"product_id\": \"src.product_id\",\n",
    "            \"seller_id\": \"src.seller_id\",            \n",
    "            \"product_category_name\": \"src.product_category_name\",\n",
    "            \"product_category_name_english\": \"src.product_category_name_english\",\n",
    "            \"price\": \"src.price\",\n",
    "            \"freight_value\": \"src.freight_value\",\n",
    "            \"product_name_length\": \"src.product_name_length\",\n",
    "            \"product_description_length\": \"src.product_description_length\",\n",
    "            \"product_photos_qty\": \"src.product_photos_qty\",\n",
    "            \"product_weight_g\": \"src.product_weight_g\",\n",
    "            \"product_length_cm\": \"src.product_length_cm\",\n",
    "            \"product_height_cm\": \"src.product_height_cm\",\n",
    "            \"product_width_cm\": \"src.product_width_cm\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcb40e7-0850-4173-89f3-0517687df4f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Sellers(Silver):\n",
    "\n",
    "    \"\"\" Method to read the parquet file from bronze staging layer with read options & return the pyspark dataframe     \n",
    "    \"\"\"\n",
    "    def read_bronze(self, spark, src_file_path, src_columns): \n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        bronze_df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{src_file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        bronze_df = (\n",
    "            bronze_df \\\n",
    "            .select(*src_columns)\n",
    "        )\n",
    "\n",
    "        return bronze_df\n",
    "    \n",
    "\n",
    "    # Method to perform transformations in the sellers bronze source pyspark dataframe\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        \n",
    "        # DATA QUALITY CHECK\n",
    "        transformed_df = bronze_df.filter(\"seller_id IS NOT NULL\")        \n",
    "\n",
    "        # DATA DEDUPLICATION\n",
    "        sellers_windowSpec = Window.partitionBy(\"seller_id\").orderBy(\"seller_id\")\n",
    "\n",
    "        transformed_df = transformed_df.withColumn(\"row_number\", row_number().over(sellers_windowSpec)) \\\n",
    "                .filter(\"row_number = 1\") \\\n",
    "                .drop(\"row_number\")\n",
    "                \n",
    "        # CORRECTING DATASTRUCTURE ISSUES\n",
    "        transformed_df = transformed_df.select(\n",
    "                        upper(trim(\"seller_id\")).alias(\"seller_id\"),\n",
    "                        initcap(trim(\"seller_state\")).alias(\"seller_state\"),\n",
    "                        upper(trim(\"seller_state_code\")).alias(\"seller_state_code\")\n",
    "                        )\n",
    "        \n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    # Method to perform upsert/merge for sellers delta table in silver layer\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "\n",
    "        # Reading sellers delta table from silver layer\n",
    "        silver_olist_sellers = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Performing upsert to the sellers delta table in silver layer\n",
    "        silver_olist_sellers.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            transformed_df.alias(\"src\"),\n",
    "            (\"tgt.seller_id = src.seller_id\")\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "            {\n",
    "            \"seller_state\": \"src.seller_state\",\n",
    "            \"seller_state_code\": \"src.seller_state_code\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "            {\n",
    "            \"seller_id\": \"src.seller_id\",\n",
    "            \"seller_state\": \"src.seller_state\",\n",
    "            \"seller_state_code\": \"src.seller_state_code\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0516bb0d-263e-45f7-a678-5e85b7389283",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Order_Items(Silver):\n",
    "\n",
    "    \"\"\" Method to read the parquet file from bronze staging layer with read options & return the pyspark dataframe     \n",
    "    \"\"\"\n",
    "    def read_bronze(self, spark, src_file_path, src_columns): \n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        bronze_df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{src_file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        bronze_df = (\n",
    "            bronze_df \\\n",
    "            .select(*src_columns)\n",
    "        )\n",
    "\n",
    "        return bronze_df\n",
    "   \n",
    "\n",
    "    # Method to perform transformations in the order_items bronze source pyspark dataframe\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        \n",
    "        # DATA QUALITY CHECK\n",
    "        transformed_df = bronze_df.filter(\"order_id is NOT NULL AND product_id is NOT NULL AND seller_id IS NOT NULL\")        \n",
    "\n",
    "        # DATA DEDUPLICATION\n",
    "        order_items_windowSpec = Window.partitionBy(\"order_id\", \"order_item_id\", \"product_id\", \"seller_id\").orderBy(\"order_id\", \"order_item_id\", \"product_id\", \"seller_id\")\n",
    "\n",
    "        transformed_df = transformed_df.withColumn(\"row_number\", row_number().over(order_items_windowSpec)) \\\n",
    "                .filter(\"row_number = 1\") \\\n",
    "                .drop(\"row_number\")\n",
    "                \n",
    "        # CORRECTING DATASTRUCTURE ISSUES\n",
    "        transformed_df = transformed_df \\\n",
    "                .withColumn(\"order_id\", upper(trim(\"order_id\"))) \\\n",
    "                .withColumn(\"product_id\", upper(trim(\"product_id\"))) \\\n",
    "                .withColumn(\"seller_id\", upper(trim(\"seller_id\"))) \\\n",
    "                .withColumn(\"shipping_limit_date\", to_timestamp(\"shipping_limit_date\", timestamp_format)) \\\n",
    "                .withColumn(\"shipping_year\", year(\"shipping_limit_date\")) \\\n",
    "                .groupBy(\n",
    "                    \"order_id\",\n",
    "                    \"product_id\",\n",
    "                    \"seller_id\",\n",
    "                    \"shipping_limit_date\",\n",
    "                    \"shipping_year\"\n",
    "                ).agg(\n",
    "                    count(\"order_item_id\").alias(\"item_quantity\"),\n",
    "                    round(sum(\"price\"), 2).alias(\"price\"),\n",
    "                    round(sum(\"freight_value\"), 2).alias(\"freight_value\")\n",
    "                )\n",
    "\n",
    "        transformed_df = transformed_df.select(\n",
    "                        \"order_id\",\n",
    "                        \"product_id\",\n",
    "                        \"seller_id\",\n",
    "                        \"shipping_limit_date\",\n",
    "                        \"shipping_year\",\n",
    "                        \"item_quantity\",\n",
    "                        \"price\",\n",
    "                        \"freight_value\"                \n",
    "                )        \n",
    "        \n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    # Method to perform upsert/merge for order_items delta table in silver layer\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "\n",
    "        # Reading order_items delta table from silver layer\n",
    "        silver_olist_order_items = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Performing upsert to the order_items delta table in silver layer\n",
    "        silver_olist_order_items.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            transformed_df.alias(\"src\"),\n",
    "            (\"\"\"\n",
    "            ((tgt.order_id = src.order_id) and \n",
    "             (tgt.product_id = src.product_id) and\n",
    "             (tgt.seller_id = src.seller_id))\n",
    "            \"\"\")\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "            {\n",
    "            \"shipping_limit_date\": \"src.shipping_limit_date\",\n",
    "            \"shipping_year\": \"src.shipping_year\",\n",
    "            \"item_quantity\": \"src.item_quantity\",\n",
    "            \"price\": \"src.price\",\n",
    "            \"freight_value\": \"src.freight_value\",        \n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "            {      \n",
    "            \"order_id\": \"src.order_id\",\n",
    "            \"product_id\": \"src.product_id\",               \n",
    "            \"seller_id\": \"src.seller_id\",                      \n",
    "            \"shipping_limit_date\": \"src.shipping_limit_date\",\n",
    "            \"shipping_year\": \"src.shipping_year\",\n",
    "            \"item_quantity\": \"src.item_quantity\",\n",
    "            \"price\": \"src.price\",\n",
    "            \"freight_value\": \"src.freight_value\",        \n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2d028b-244a-4cc1-891f-13e9812dfe8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Order_Ratings(Silver):\n",
    "\n",
    "    \"\"\" Method to read the parquet file from bronze staging layer with read options & return the pyspark dataframe     \n",
    "    \"\"\"\n",
    "    def read_bronze(self, spark, src_file_path, src_columns): \n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        bronze_df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{src_file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        bronze_df = (\n",
    "            bronze_df \\\n",
    "            .select(*src_columns)\n",
    "        )\n",
    "\n",
    "        return bronze_df\n",
    "    \n",
    "\n",
    "    # Method to perform transformations in the ratings bronze source pyspark dataframe\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        \n",
    "        # DATA QUALITY CHECK\n",
    "        transformed_df = bronze_df.filter(\"rating_id is NOT NULL AND order_id is NOT NULL\")        \n",
    "\n",
    "        # DATA DEDUPLICATION\n",
    "        order_ratings_windowSpec = Window.partitionBy(\"rating_id\", \"order_id\").orderBy(\"rating_id\", \"order_id\")\n",
    "\n",
    "        transformed_df = transformed_df.withColumn(\"row_number\", row_number().over(order_ratings_windowSpec)) \\\n",
    "                .filter(\"row_number = 1\") \\\n",
    "                .drop(\"row_number\")\n",
    "                \n",
    "        # CORRECTING DATASTRUCTURE ISSUES\n",
    "        transformed_df = transformed_df.select(\n",
    "                        upper(trim(\"rating_id\")).alias(\"rating_id\"),    \n",
    "                        upper(trim(\"order_id\")).alias(\"order_id\"),\n",
    "                        col(\"rating_score\").cast(\"int\").alias(\"rating_score\"),\n",
    "                        to_date(to_timestamp(\"rating_survey_creation_date\", timestamp_format), \"yyyy-MM-dd\").alias(\"rating_survey_creation_date\"),\n",
    "                        to_timestamp(\"rating_survey_answer_timestamp\", timestamp_format).alias(\"rating_survey_answer_timestamp\")\n",
    "                        )\n",
    "        \n",
    "        # rating_score validation\n",
    "        transformed_df = transformed_df.filter(col(\"rating_score\").between(1,5))\n",
    "        \n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    # Method to perform upsert/merge for ratings delta table in silver layer\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "\n",
    "        # Reading ratings delta table from silver layer\n",
    "        silver_olist_order_ratings = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Performing upsert to the ratings delta table in silver layer\n",
    "        silver_olist_order_ratings.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            transformed_df.alias(\"src\"),\n",
    "            (\"\"\"\n",
    "            ((tgt.order_id = src.order_id) and \n",
    "             (tgt.rating_id = src.rating_id))\n",
    "            \"\"\")\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "            {\n",
    "            \"rating_score\": \"src.rating_score\",\n",
    "            \"rating_survey_creation_date\": \"src.rating_survey_creation_date\",\n",
    "            \"rating_survey_answer_timestamp\": \"src.rating_survey_answer_timestamp\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "            {\n",
    "            \"rating_id\": \"src.rating_id\",                \n",
    "            \"order_id\": \"src.order_id\",\n",
    "            \"rating_score\": \"src.rating_score\",\n",
    "            \"rating_survey_creation_date\": \"src.rating_survey_creation_date\",\n",
    "            \"rating_survey_answer_timestamp\": \"src.rating_survey_answer_timestamp\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcf52aa-e0a2-41e4-8e94-8f7bd8a07efc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Order_Payments(Silver):\n",
    "\n",
    "    \"\"\" Method to read the parquet file from bronze staging layer with read options & return the pyspark dataframe     \n",
    "    \"\"\"\n",
    "    def read_bronze(self, spark, src_file_path, src_columns): \n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        bronze_df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{src_file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        bronze_df = (\n",
    "            bronze_df \\\n",
    "            .select(*src_columns)\n",
    "        )\n",
    "\n",
    "        return bronze_df\n",
    "    \n",
    "\n",
    "    # Method to perform transformations in the payments bronze source pyspark dataframe\n",
    "    def transform_bronze_df(self, bronze_df, timestamp_format):\n",
    "        \n",
    "        # DATA QUALITY CHECK\n",
    "        transformed_df = bronze_df.filter(\"order_id is NOT NULL AND payment_sequential is NOT NULL\")        \n",
    "\n",
    "        # DATA DEDUPLICATION\n",
    "        order_payments_windowSpec = Window.partitionBy(\"order_id\", \"payment_sequential\").orderBy(\"order_id\", \"payment_sequential\")\n",
    "\n",
    "        transformed_df = transformed_df.withColumn(\"row_number\", row_number().over(order_payments_windowSpec)) \\\n",
    "                .filter(\"row_number = 1\") \\\n",
    "                .drop(\"row_number\")\n",
    "                \n",
    "        # CORRECTING DATASTRUCTURE ISSUES\n",
    "        transformed_df = transformed_df.select(\n",
    "                        upper(trim(\"order_id\")).alias(\"order_id\"),\n",
    "                        col(\"payment_sequential\").cast(\"int\").alias(\"payment_sequential\"),\n",
    "                        initcap(trim(translate(\"payment_type\", \"_\", \" \"))).alias(\"payment_type\"),\n",
    "                        col(\"payment_value\").cast(\"float\").alias(\"payment_value\")\n",
    "                        )\n",
    "        \n",
    "        return transformed_df\n",
    "\n",
    "\n",
    "    # Method to perform upsert/merge for payments delta table in silver layer\n",
    "    def load_silver(self, spark, basePath, transformed_df):\n",
    "\n",
    "        # Reading payments delta table from silver layer\n",
    "        silver_olist_order_payments = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Performing upsert to the payments delta table in silver layer\n",
    "        silver_olist_order_payments.alias(\"tgt\") \\\n",
    "        .merge(\n",
    "            transformed_df.alias(\"src\"),\n",
    "            (\"\"\"\n",
    "            ((tgt.order_id = src.order_id) and \n",
    "             (tgt.payment_sequential = src.payment_sequential))\n",
    "            \"\"\")\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "            {\n",
    "            \"payment_sequential\": \"src.payment_sequential\",\n",
    "            \"payment_type\": \"src.payment_type\",\n",
    "            \"payment_value\": \"src.payment_value\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "            {\n",
    "            \"order_id\": \"src.order_id\",\n",
    "            \"payment_sequential\": \"src.payment_sequential\",\n",
    "            \"payment_type\": \"src.payment_type\",\n",
    "            \"payment_value\": \"src.payment_value\",\n",
    "            \"last_update_date\": to_date(current_timestamp(), \"yyyy-MM-dd\")\n",
    "            }\n",
    "        ) \\\n",
    "        .execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052bdcc9-f996-45d4-a7b8-fc97c330ca44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **4. Factory class with static method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fae9a9d-2c2f-4d75-ab0e-2de75fdff913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Factory class with static method\n",
    "class Factory:\n",
    "\n",
    "    # Method to return the instance based on the application\n",
    "    @staticmethod\n",
    "    def get_etl(app_nm: str) -> Silver:\n",
    "\n",
    "        apps = {\n",
    "        \"Customers\" : Customers(),\n",
    "        \"Orders\" : Orders(),\n",
    "        \"Products\" : Products(),\n",
    "        \"Sellers\" : Sellers(),\n",
    "        \"Order_Items\" : Order_Items(),\n",
    "        \"Order_Ratings\" : Order_Ratings(),\n",
    "        \"Order_Payments\" : Order_Payments()\n",
    "        }\n",
    "\n",
    "        while True:\n",
    "            # If application is present in the apps dictionary it'll return the object of the concrete class\n",
    "            if app_nm in apps:\n",
    "                return apps[app_nm]\n",
    "            \n",
    "            print(f\"Unknown application : {app_nm}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
