{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897a46eb-8bd3-43d2-a8bc-4a1df06864f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **1. Importing the required modules and functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4613a2b4-ee1a-42e7-9aba-bf9119978620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required modules and functions\n",
    "from pyspark.sql.functions import lit, current_date, concat\n",
    "from delta.tables import DeltaTable\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "529c9871-37a3-4424-a854-888f55a428b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **2. Abstract base class for factory interface.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b000c9-4efa-4cb2-be7a-1fc707741193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating abstract base class for factory interface\n",
    "class Gold_Dims(ABC):\n",
    "    # Basic representation of the data extraction & loading codes\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_silver(self, spark, src_table_path):\n",
    "        # Read and returns a pyspark dataframe from the Delta tables in Silver layer\n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_dimension(self, spark, basePath, silver_df):\n",
    "        # Performs SCD2 using merge for the dimension Delta tables in Gold layer with silver pyspark dataframe \n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c6c15f-24e9-4507-9240-0b042311886e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **3. Concrete classes and implementing the abstract methods in subclasses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366b9a01-033c-48ed-9183-4bf63f043921",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Customers(Gold_Dims):\n",
    "\n",
    "    # Method to read the Delta tables file from Silver layer & return the pyspark dataframe     \n",
    "    def read_silver(self, spark, src_table_path): \n",
    "        \n",
    "        # Reading the Delta file from Silver layer\n",
    "        silver_delta = DeltaTable.forPath(spark, f\"abfss://{src_table_path}\")\n",
    "\n",
    "        # Converting the delta table to pyspark dataframe\n",
    "        silver_df = silver_delta.toDF()\n",
    "\n",
    "        return silver_df\n",
    "    \n",
    "\n",
    "    # Method to perform SCD2 using merge for customers dimension delta table in gold layer\n",
    "    def load_dimension(self, spark, basePath, silver_df):\n",
    "\n",
    "        # Reading customers dimensions delta table from gold layer\n",
    "        dim_olist_customers = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Rows to INSERT new rows of existing customers\n",
    "        newRowsToInsert = silver_df.alias(\"updates\") \\\n",
    "        .join(dim_olist_customers.toDF().alias(\"customers\"), \"customer_id\") \\\n",
    "        .where(\"\"\"\n",
    "               customers.is_current = 1 AND \n",
    "               ((updates.customer_email_id <> customers.customer_email_id) or \n",
    "                (updates.customer_state <> customers.customer_state) or \n",
    "                (updates.customer_state_code <> customers.customer_state_code))\n",
    "                \"\"\")\n",
    "        \n",
    "        # Stage the update by unioning two sets of rows\n",
    "        # 1. Rows that will be inserted in the whenNotMatched clause\n",
    "        # 2. Rows that will either update the current changes of existing customers or insert the data of new customers\n",
    "        stagedUpdates = (\n",
    "        newRowsToInsert\n",
    "        .selectExpr(\"NULL as mergeKey\", \"updates.*\")   # Rows for 1\n",
    "        .union(silver_df.selectExpr(\"customer_id as mergeKey\", \"*\"))  # Rows for 2.\n",
    "        )\n",
    "\n",
    "        # Apply SCD Type 2 operation using merge\n",
    "        dim_olist_customers.alias(\"customers\").merge(\n",
    "        stagedUpdates.alias(\"staged_updates\"),\n",
    "        \"customers.customer_id = mergeKey\") \\\n",
    "        .whenMatchedUpdate(\n",
    "        condition = \"\"\"customers.is_current = 1 AND                \n",
    "                    ((staged_updates.customer_email_id <> customers.customer_email_id) or \n",
    "                     (staged_updates.customer_state <> customers.customer_state) or \n",
    "                     (staged_updates.customer_state_code <> customers.customer_state_code)\n",
    "                     )\"\"\",\n",
    "        set = { # Set is_current to 0 and eff_end_dt to current date - 1.\n",
    "            \"is_current\": lit(0),\n",
    "            \"eff_end_dt\": (current_date() - 1),\n",
    "            \"update_date\": current_date(),\n",
    "            \"updated_by\": lit(\"olist_etl\")            \n",
    "        }\n",
    "        ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"customer_skey\": concat(\"staged_updates.customer_id\", lit(\"_skey\")),\n",
    "            \"customer_id\": \"staged_updates.customer_id\",\n",
    "            \"customer_email_id\": \"staged_updates.customer_email_id\",\n",
    "            \"customer_state\": \"staged_updates.customer_state\",\n",
    "            \"customer_state_code\": \"staged_updates.customer_state_code\",\n",
    "            \"eff_start_dt\": current_date(), # Set is_current to 1 along with the new rows and its eff_start_dt.\n",
    "            \"eff_end_dt\": lit(\"9999-12-31\").cast(\"date\"),\n",
    "            \"is_current\": lit(1),\n",
    "            \"insert_date\": current_date(),\n",
    "            \"inserted_by\": lit(\"olist_etl\"),\n",
    "            \"update_date\": lit(None).cast(\"string\"),\n",
    "            \"updated_by\": lit(None).cast(\"string\")\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0fb63e5-25a4-414f-97e2-bbab458e337c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Orders(Gold_Dims):\n",
    "\n",
    "    # Method to read the Delta table from Silver layer & return the pyspark dataframe     \n",
    "    def read_silver(self, spark, src_table_path): \n",
    "        \n",
    "        # Reading the Delta file from Silver layer\n",
    "        silver_delta = DeltaTable.forPath(spark, f\"abfss://{src_table_path}\")\n",
    "\n",
    "        # Converting the delta table to pyspark dataframe\n",
    "        silver_df = silver_delta.toDF()\n",
    "\n",
    "        return silver_df\n",
    "    \n",
    "\n",
    "    # Method to perform SCD2 using merge for orders dimension delta table in gold layer\n",
    "    def load_dimension(self, spark, basePath, silver_df):\n",
    "\n",
    "        # Reading orders dimensions delta table from gold layer\n",
    "        dim_olist_orders = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Rows to INSERT new rows of existing orders\n",
    "        newRowsToInsert = silver_df.alias(\"updates\") \\\n",
    "        .join(dim_olist_orders.toDF().alias(\"orders\"), [\"order_id\", \"customer_id\"]) \\\n",
    "        .where(\"\"\"\n",
    "               orders.is_current = 1 AND \n",
    "               ((updates.order_status <> orders.order_status) or \n",
    "                (updates.order_purchase_timestamp <> orders.order_purchase_timestamp) or \n",
    "                (updates.order_approved_at <> orders.order_approved_at) or\n",
    "                (updates.order_delivered_carrier_date <> orders.order_delivered_carrier_date) or \n",
    "                (updates.order_delivered_customer_date <> orders.order_delivered_customer_date) or\n",
    "                (updates.order_estimated_delivery_date <> orders.order_estimated_delivery_date))\n",
    "                \"\"\")\n",
    "        \n",
    "        # Stage the update by unioning two sets of rows\n",
    "        # 1. Rows that will be inserted in the whenNotMatched clause\n",
    "        # 2. Rows that will either update the current changes of existing orders or insert the data of new orders\n",
    "        stagedUpdates = (\n",
    "        newRowsToInsert\n",
    "        .selectExpr(\"NULL as mergeKey\", \"updates.*\")   # Rows for 1\n",
    "        .union(silver_df.selectExpr(\"order_id as mergeKey\", \"*\"))  # Rows for 2.\n",
    "        )\n",
    "\n",
    "        # Apply SCD Type 2 operation using merge\n",
    "        dim_olist_orders.alias(\"orders\").merge(\n",
    "        stagedUpdates.alias(\"staged_updates\"),\n",
    "        \"orders.order_id = mergeKey\") \\\n",
    "        .whenMatchedUpdate(\n",
    "        condition = \"\"\"orders.is_current = 1 AND                \n",
    "                    ((staged_updates.order_status <> orders.order_status) or \n",
    "                    (staged_updates.order_purchase_timestamp <> orders.order_purchase_timestamp) or \n",
    "                    (staged_updates.order_approved_at <> orders.order_approved_at) or\n",
    "                    (staged_updates.order_delivered_carrier_date <> orders.order_delivered_carrier_date) or \n",
    "                    (staged_updates.order_delivered_customer_date <> orders.order_delivered_customer_date) or\n",
    "                    (staged_updates.order_estimated_delivery_date <> orders.order_estimated_delivery_date)\n",
    "                     )\"\"\",\n",
    "        set = { # Set is_current to 0 and eff_end_dt to current date - 1.\n",
    "            \"is_current\": lit(0),\n",
    "            \"eff_end_dt\": (current_date() - 1),\n",
    "            \"update_date\": current_date(),\n",
    "            \"updated_by\": lit(\"olist_etl\")            \n",
    "        }\n",
    "        ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"order_skey\": concat(\"staged_updates.order_id\", lit(\"_skey\")),\n",
    "            \"order_id\": \"staged_updates.order_id\",\n",
    "            \"customer_id\": \"staged_updates.customer_id\",\n",
    "            \"order_status\": \"staged_updates.order_status\",\n",
    "            \"order_purchase_timestamp\": \"staged_updates.order_purchase_timestamp\",\n",
    "            \"order_approved_at\": \"staged_updates.order_approved_at\",\n",
    "            \"order_delivered_carrier_date\": \"staged_updates.order_delivered_carrier_date\",\n",
    "            \"order_delivered_customer_date\": \"staged_updates.order_delivered_customer_date\",\n",
    "            \"order_estimated_delivery_date\": \"staged_updates.order_estimated_delivery_date\",\n",
    "            \"eff_start_dt\": current_date(), # Set is_current to 1 along with the new rows and its eff_start_dt.\n",
    "            \"eff_end_dt\": lit(\"9999-12-31\").cast(\"date\"),\n",
    "            \"is_current\": lit(1),\n",
    "            \"insert_date\": current_date(),\n",
    "            \"inserted_by\": lit(\"olist_etl\"),\n",
    "            \"update_date\": lit(None).cast(\"string\"),\n",
    "            \"updated_by\": lit(None).cast(\"string\")\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c54681-7294-462c-882b-99a906a78384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Products(Gold_Dims):\n",
    "\n",
    "    # Method to read the Delta table from Silver layer & return the pyspark dataframe     \n",
    "    def read_silver(self, spark, src_table_path): \n",
    "        \n",
    "        # Reading the Delta file from Silver layer\n",
    "        silver_delta = DeltaTable.forPath(spark, f\"abfss://{src_table_path}\")\n",
    "\n",
    "        # Converting the delta table to pyspark dataframe\n",
    "        silver_df = silver_delta.toDF()\n",
    "\n",
    "        return silver_df\n",
    "    \n",
    "\n",
    "    # Method to perform SCD2 using merge for products dimension delta table in gold layer\n",
    "    def load_dimension(self, spark, basePath, silver_df):\n",
    "\n",
    "        # Reading products dimensions delta table from gold layer\n",
    "        dim_olist_products = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Rows to INSERT new rows of existing products\n",
    "        newRowsToInsert = silver_df.alias(\"updates\") \\\n",
    "        .join(dim_olist_products.toDF().alias(\"products\"), [\"product_id\", \"seller_id\"]) \\\n",
    "        .where(\"\"\"\n",
    "               products.is_current = 1 AND \n",
    "               ((updates.product_category_name <> products.product_category_name) or \n",
    "                (updates.product_category_name_english <> products.product_category_name_english) or \n",
    "                (updates.price <> products.price) or\n",
    "                (updates.freight_value <> products.freight_value) or \n",
    "                (updates.product_name_length <> products.product_name_length) or\n",
    "                (updates.product_description_length <> products.product_description_length) or\n",
    "                (updates.product_photos_qty <> products.product_photos_qty) or \n",
    "                (updates.product_weight_g <> products.product_weight_g) or\n",
    "                (updates.product_length_cm <> products.product_length_cm) or \n",
    "                (updates.product_height_cm <> products.product_height_cm) or\n",
    "                (updates.product_width_cm <> products.product_width_cm))\n",
    "                \"\"\")\n",
    "        \n",
    "        # Stage the update by unioning two sets of rows\n",
    "        # 1. Rows that will be inserted in the whenNotMatched clause\n",
    "        # 2. Rows that will either update the current changes of existing products or insert the new data of new products\n",
    "        stagedUpdates = (\n",
    "        newRowsToInsert\n",
    "        .selectExpr(\"NULL as mergeKey\", \"updates.*\")   # Rows for 1\n",
    "        .union(silver_df.selectExpr(\"product_id as mergeKey\", \"*\"))  # Rows for 2.\n",
    "        )\n",
    "\n",
    "        # Apply SCD Type 2 operation using merge\n",
    "        dim_olist_products.alias(\"products\").merge(\n",
    "        stagedUpdates.alias(\"staged_updates\"),\n",
    "        \"products.product_id = mergeKey\") \\\n",
    "        .whenMatchedUpdate(\n",
    "        condition = \"\"\"products.is_current = 1 AND                \n",
    "                    ((staged_updates.product_category_name <> products.product_category_name) or \n",
    "                    (staged_updates.product_category_name_english <> products.product_category_name_english) or \n",
    "                    (staged_updates.price <> products.price) or\n",
    "                    (staged_updates.freight_value <> products.freight_value) or \n",
    "                    (staged_updates.product_name_length <> products.product_name_length) or\n",
    "                    (staged_updates.product_description_length <> products.product_description_length) or\n",
    "                    (staged_updates.product_photos_qty <> products.product_photos_qty) or \n",
    "                    (staged_updates.product_weight_g <> products.product_weight_g) or\n",
    "                    (staged_updates.product_length_cm <> products.product_length_cm) or \n",
    "                    (staged_updates.product_height_cm <> products.product_height_cm) or\n",
    "                    (staged_updates.product_width_cm <> products.product_width_cm)\n",
    "                     )\"\"\",\n",
    "        set = { # Set is_current to 0 and eff_end_dt to current date - 1.\n",
    "            \"is_current\": lit(0),\n",
    "            \"eff_end_dt\": (current_date() - 1),\n",
    "            \"update_date\": current_date(),\n",
    "            \"updated_by\": lit(\"olist_etl\")            \n",
    "        }\n",
    "        ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"product_skey\": concat(\"staged_updates.product_id\", lit(\"_skey\")),\n",
    "            \"product_id\": \"staged_updates.product_id\",\n",
    "            \"seller_id\": \"staged_updates.seller_id\",\n",
    "            \"product_category_name\": \"staged_updates.product_category_name\",\n",
    "            \"product_category_name_english\": \"staged_updates.product_category_name_english\",\n",
    "            \"price\": \"staged_updates.price\",\n",
    "            \"freight_value\": \"staged_updates.freight_value\",\n",
    "            \"product_name_length\": \"staged_updates.product_name_length\",\n",
    "            \"product_description_length\": \"staged_updates.product_description_length\",\n",
    "            \"product_photos_qty\": \"staged_updates.product_photos_qty\",\n",
    "            \"product_weight_g\": \"staged_updates.product_weight_g\",\n",
    "            \"product_length_cm\": \"staged_updates.product_length_cm\",\n",
    "            \"product_height_cm\": \"staged_updates.product_height_cm\",\n",
    "            \"product_width_cm\": \"staged_updates.product_width_cm\",                        \n",
    "            \"eff_start_dt\": current_date(), # Set is_current to 1 along with the new rows and its eff_start_dt.\n",
    "            \"eff_end_dt\": lit(\"9999-12-31\").cast(\"date\"),\n",
    "            \"is_current\": lit(1),\n",
    "            \"insert_date\": current_date(),\n",
    "            \"inserted_by\": lit(\"olist_etl\"),\n",
    "            \"update_date\": lit(None).cast(\"string\"),\n",
    "            \"updated_by\": lit(None).cast(\"string\")\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf7c413-fadf-4a0e-8720-c078b8fd090a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Sellers(Gold_Dims):\n",
    "\n",
    "    # Method to read the Delta table from Silver layer & return the pyspark dataframe     \n",
    "    def read_silver(self, spark, src_table_path): \n",
    "        \n",
    "        # Reading the Delta file from Silver layer\n",
    "        silver_delta = DeltaTable.forPath(spark, f\"abfss://{src_table_path}\")\n",
    "\n",
    "        # Converting the delta table to pyspark dataframe\n",
    "        silver_df = silver_delta.toDF()\n",
    "\n",
    "        return silver_df\n",
    "    \n",
    "\n",
    "    # Method to perform SCD2 using merge for sellers dimension delta table in gold layer\n",
    "    def load_dimension(self, spark, basePath, silver_df):\n",
    "\n",
    "        # Reading sellers dimensions delta table from gold layer\n",
    "        dim_olist_sellers = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Rows to INSERT new rows of existing customers\n",
    "        newRowsToInsert = silver_df.alias(\"updates\") \\\n",
    "        .join(dim_olist_sellers.toDF().alias(\"sellers\"), \"seller_id\") \\\n",
    "        .where(\"\"\"\n",
    "               sellers.is_current = 1 AND \n",
    "               ((updates.seller_state <> sellers.seller_state) or \n",
    "                (updates.seller_state_code <> sellers.seller_state_code))\n",
    "                \"\"\")\n",
    "        \n",
    "        # Stage the update by unioning two sets of rows\n",
    "        # 1. Rows that will be inserted in the whenNotMatched clause\n",
    "        # 2. Rows that will either update the current changes of existing sellers or insert the new data of new sellers\n",
    "        stagedUpdates = (\n",
    "        newRowsToInsert\n",
    "        .selectExpr(\"NULL as mergeKey\", \"updates.*\")   # Rows for 1\n",
    "        .union(silver_df.selectExpr(\"seller_id as mergeKey\", \"*\"))  # Rows for 2.\n",
    "        )\n",
    "\n",
    "        # Apply SCD Type 2 operation using merge\n",
    "        dim_olist_sellers.alias(\"sellers\").merge(\n",
    "        stagedUpdates.alias(\"staged_updates\"),\n",
    "        \"sellers.seller_id = mergeKey\") \\\n",
    "        .whenMatchedUpdate(\n",
    "        condition = \"\"\"sellers.is_current = 1 AND                \n",
    "                    ((staged_updates.seller_state <> sellers.seller_state) or \n",
    "                    (staged_updates.seller_state_code <> sellers.seller_state_code)\n",
    "                     )\"\"\",\n",
    "        set = { # Set is_current to 0 and eff_end_dt to current date - 1.\n",
    "            \"is_current\": lit(0),\n",
    "            \"eff_end_dt\": (current_date() - 1),\n",
    "            \"update_date\": current_date(),\n",
    "            \"updated_by\": lit(\"olist_etl\")            \n",
    "        }\n",
    "        ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"seller_skey\": concat(\"staged_updates.seller_id\", lit(\"_skey\")),\n",
    "            \"seller_id\": \"staged_updates.seller_id\",\n",
    "            \"seller_state\": \"staged_updates.seller_state\",\n",
    "            \"seller_state_code\": \"staged_updates.seller_state_code\",\n",
    "            \"eff_start_dt\": current_date(), # Set is_current to 1 along with the new rows and its eff_start_dt.\n",
    "            \"eff_end_dt\": lit(\"9999-12-31\").cast(\"date\"),\n",
    "            \"is_current\": lit(1),\n",
    "            \"insert_date\": current_date(),\n",
    "            \"inserted_by\": lit(\"olist_etl\"),\n",
    "            \"update_date\": lit(None).cast(\"string\"),\n",
    "            \"updated_by\": lit(None).cast(\"string\")\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b597925c-b038-423d-baab-d7a1c977b0bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Order_Ratings(Gold_Dims):\n",
    "\n",
    "    # Method to read the Delta table from Silver layer & return the pyspark dataframe     \n",
    "    def read_silver(self, spark, src_table_path): \n",
    "        \n",
    "        # Reading the Delta file from Silver layer\n",
    "        silver_delta = DeltaTable.forPath(spark, f\"abfss://{src_table_path}\")\n",
    "\n",
    "        # Converting the delta table to pyspark dataframe\n",
    "        silver_df = silver_delta.toDF()\n",
    "\n",
    "        return silver_df\n",
    "    \n",
    "\n",
    "    # Method to perform SCD2 using merge for order_ratings dimension delta table in gold layer\n",
    "    def load_dimension(self, spark, basePath, silver_df):\n",
    "\n",
    "        # Reading order_ratings dimensions delta table from gold layer\n",
    "        dim_olist_order_ratings = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Rows to INSERT new rows of existing order_ratings\n",
    "        newRowsToInsert = silver_df.alias(\"updates\") \\\n",
    "        .join(dim_olist_order_ratings.toDF().alias(\"order_ratings\"), [\"rating_id\", \"order_id\"]) \\\n",
    "        .where(\"\"\"\n",
    "               order_ratings.is_current = 1 AND \n",
    "               ((updates.rating_score <> order_ratings.rating_score) or \n",
    "                (updates.rating_survey_creation_date <> order_ratings.rating_survey_creation_date) or \n",
    "                (updates.rating_survey_answer_timestamp <> order_ratings.rating_survey_answer_timestamp))\n",
    "                \"\"\")\n",
    "        \n",
    "        # Stage the update by unioning two sets of rows\n",
    "        # 1. Rows that will be inserted in the whenNotMatched clause\n",
    "        # 2. Rows that will either update the current changes of existing ratings or insert the new data of new ratings\n",
    "        stagedUpdates = (\n",
    "        newRowsToInsert\n",
    "        .selectExpr(\"NULL as mergeKey\", \"updates.*\")   # Rows for 1\n",
    "        .union(silver_df.selectExpr(\"rating_id as mergeKey\", \"*\"))  # Rows for 2.\n",
    "        )\n",
    "\n",
    "        # Apply SCD Type 2 operation using merge\n",
    "        dim_olist_order_ratings.alias(\"order_ratings\").merge(\n",
    "        stagedUpdates.alias(\"staged_updates\"),\n",
    "        \"order_ratings.rating_id = mergeKey\") \\\n",
    "        .whenMatchedUpdate(\n",
    "        condition = \"\"\"order_ratings.is_current = 1 AND                \n",
    "                    ((staged_updates.rating_score <> order_ratings.rating_score) or \n",
    "                    (staged_updates.rating_survey_creation_date <> order_ratings.rating_survey_creation_date) or \n",
    "                    (staged_updates.rating_survey_answer_timestamp <> order_ratings.rating_survey_answer_timestamp)\n",
    "                     )\"\"\",\n",
    "        set = { # Set is_current to 0 and eff_end_dt to current date - 1.\n",
    "            \"is_current\": lit(0),\n",
    "            \"eff_end_dt\": (current_date() - 1),\n",
    "            \"update_date\": current_date(),\n",
    "            \"updated_by\": lit(\"olist_etl\")            \n",
    "        }\n",
    "        ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"rating_skey\": concat(\"staged_updates.rating_id\", lit(\"_skey\")),\n",
    "            \"rating_id\": \"staged_updates.rating_id\",\n",
    "            \"order_id\": \"staged_updates.order_id\",\n",
    "            \"rating_score\": \"staged_updates.rating_score\",\n",
    "            \"rating_survey_creation_date\": \"staged_updates.rating_survey_creation_date\",\n",
    "            \"rating_survey_answer_timestamp\": \"staged_updates.rating_survey_answer_timestamp\",\n",
    "            \"eff_start_dt\": current_date(), # Set is_current to 1 along with the new rows and its eff_start_dt.\n",
    "            \"eff_end_dt\": lit(\"9999-12-31\").cast(\"date\"),\n",
    "            \"is_current\": lit(1),\n",
    "            \"insert_date\": current_date(),\n",
    "            \"inserted_by\": lit(\"olist_etl\"),\n",
    "            \"update_date\": lit(None).cast(\"string\"),\n",
    "            \"updated_by\": lit(None).cast(\"string\")\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052bdcc9-f996-45d4-a7b8-fc97c330ca44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **4. Factory class with static method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fae9a9d-2c2f-4d75-ab0e-2de75fdff913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Factory class with static method\n",
    "class Factory:\n",
    "\n",
    "    # Method to return the instance based on the target dimension table\n",
    "    @staticmethod\n",
    "    def get_gold_dims(tgt_table_name: str) -> Gold_Dims:\n",
    "\n",
    "        tables = {\n",
    "        \"dim_olist_customers\" : Customers(),\n",
    "        \"dim_olist_orders\" : Orders(),\n",
    "        \"dim_olist_products\" : Products(),\n",
    "        \"dim_olist_sellers\" : Sellers(),\n",
    "        \"dim_olist_order_ratings\" : Order_Ratings()\n",
    "        }\n",
    "\n",
    "        while True:\n",
    "            # If target table name is present in the table dictionary it'll return the object of the concrete class\n",
    "            if tgt_table_name in tables:\n",
    "                return tables[tgt_table_name]\n",
    "            \n",
    "            print(f\"Unknown target table : {tgt_table_name}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
