{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54f8b57-2620-40de-84d1-32180b9c0a43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **1. Importing required pyspark and user defined modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4f1bb5-81f7-40aa-a99b-b19dc806b2b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing AnalysisException for handling exceptions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7140cf-2324-4d96-a34d-bbc1f5c1ea9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./custom_logging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1a66f8-e313-4115-a216-b79c0de846ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9852923-16e8-486d-963a-f70faf3c3b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./spark_utils\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05cc6f1-20a4-4a31-a103-cf9d0e99353b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **2. Creating widgets to receive the parameters from ADF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29d1607-5dc0-4a21-ab7a-5dd68cd0cea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating widgets to receive the parameters from ADF\n",
    "dbutils.widgets.text(\"job_name\", \"\")\n",
    "dbutils.widgets.text(\"file_location\", \"\")\n",
    "dbutils.widgets.text(\"log_target_dir\", \"\")\n",
    "\n",
    "job_name = dbutils.widgets.get(\"job_name\")\n",
    "file_location = dbutils.widgets.get(\"file_location\")\n",
    "log_target_dir = dbutils.widgets.get(\"log_target_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d241cb-7006-480f-a0cf-d0e588711f41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **3. Configuring the custom logger from custom_logging module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b597b03b-b652-46d1-ba59-e32eb97e90e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating object for the CustomLogger class\n",
    "cust_log = CustomLogger()\n",
    "logger, log_file = cust_log.custom_logger(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99674be-94ac-4064-be51-951a5968d001",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **4. Staring pipeline log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf6557a-6d76-47f7-93b5-1f0eb27d812e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline log\n",
    "logger.info(f\"JOB_NAME : {job_name}\")\n",
    "logger.info(f\"The {job_name} for gold dimensions delta data load is started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "869fc76e-601e-42e7-a58d-7a93aa0c30ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **5.  Establish SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce4cdf5-fbbb-42b8-b03a-289cd671c1dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Establishing sparksession with service principal authentication configs.\")\n",
    "\n",
    "try:\n",
    "    spark = (\n",
    "            SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Olist_Gold_Dims\") \\\n",
    "            .config(\"fs.azure.account.auth.type.<storage-account-name>.dfs.core.windows.net\", \"OAuth\") \\\n",
    "            .config(\"fs.azure.account.oauth.provider.type.<storage-account-name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n",
    "            .config(\"fs.azure.account.oauth2.client.id.<storage-account-name>.dfs.core.windows.net\", dbutils.secrets.get(scope=\"<secret-scope-name>\", key=\"sp-client-id\")) \\\n",
    "            .config(\"fs.azure.account.oauth2.client.secret.<storage-account-name>.dfs.core.windows.net\", dbutils.secrets.get(scope=\"<secret-scope-name>\", key=\"sp-client-pwd\")) \\\n",
    "            .config(\"fs.azure.account.oauth2.client.endpoint.<storage-account-name>.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope='<secret-scope-name>', key='sp-directory-id')}/oauth2/token\") \\\n",
    "            .getOrCreate()\n",
    "        )\n",
    "except Exception as e:\n",
    "    # Logging Exception details\n",
    "    logger.critical(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "logger.info(\"Sparksession with service principal authentication configs established successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01cf3504-019a-4fba-826a-5fcd978669d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **6. Main function definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d85177b-e6ae-41d1-bd7f-8cb439a256e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    logger.info(f\"In the {__name__} function.\")\n",
    "\n",
    "    # Creating metadata variable from the user defined modules\n",
    "    logger.info(\"Creating metadata variable from the user defined modules.\")\n",
    "\n",
    "    meta_data = ReadJson(file_location)\n",
    "    gold_dims_meta = meta_data.get_metadata()\n",
    "\n",
    "    logger.info(\"Variable created successfully.\")\n",
    "\n",
    "\n",
    "    # Starting the Gold dims delta table data load\n",
    "    logger.info(\"Starting data load for the dimension delta tables in Gold layer.\")\n",
    "\n",
    "    # Looping through the gold dims metadata variable 'gold_dims_meta' for performing the data load\n",
    "    for item in gold_dims_meta:\n",
    "\n",
    "        # Pipeline variables from gold_dims_meta metadata variable\n",
    "        app_nm = item[\"app_nm\"]\n",
    "        src_table_name = item[\"src_table_name\"]\n",
    "        src_table_path = item[\"src_table_path\"]\n",
    "        tgt_table_name = item[\"tgt_table_name\"]\n",
    "        basePath = item[\"basePath\"]\n",
    "\n",
    "        logger.info(f\"Data load started for dimension table '{tgt_table_name}'.\")\n",
    "\n",
    "        logger.info(f\"\"\"\n",
    "                        app_nm      :   {app_nm}\n",
    "                        src_table_name   :   {src_table_name}\n",
    "                        src_table_path   :   {src_table_path}\n",
    "                        tgt_table_name  :   {tgt_table_name}\n",
    "                        basePath    :   {basePath}     \n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "        try: \n",
    "            # Calling the factory method from spark_utils module to instantiate the object based on the tgt_table_name\n",
    "            logger.info(f\"Calling the factory method from spark_utils module to instantiate the object based on the target table name '{tgt_table_name}'.\")            \n",
    "            factory = Factory.get_gold_dims(tgt_table_name)\n",
    "            logger.info(f\"Object instantiated successfully based on the target table name '{tgt_table_name}'.\")                        \n",
    "\n",
    "\n",
    "            # Creating pyspark dataframe from silver delta table\n",
    "            logger.info(f\"Creating pyspark dataframe from silver delta table '{src_table_name}'.\")\n",
    "            silver_df = factory.read_silver(spark, src_table_path)\n",
    "            logger.info(f\"Dataframe has been created successfully from silver delta table '{src_table_name}'.\")\n",
    "\n",
    "\n",
    "            # Performing SCD2/merge data load to the dimension delta table in Gold layer\n",
    "            logger.info(f\"Performing SCD2 data load using merge to the dimension delta table '{tgt_table_name}' in Gold layer.\")\n",
    "            factory.load_dimension(spark, basePath, silver_df)\n",
    "            logger.info(f\"Data loaded successfully to the Gold dimension delta table '{tgt_table_name}'.\")\n",
    "\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            # Logging AnalysisException details\n",
    "            logger.error(f\"AnalysisException Message : {str(e)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Logging Exception details\n",
    "            logger.critical(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "    # Shutting down the logger\n",
    "    logger.info(\"Data load for the Gold dimension delta tables has been completed.\")\n",
    "    logging.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e77f5a-5c19-4947-86a4-e2b2ab9dafae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **7. Executing the main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f5af71-ce75-4d0f-b919-07b915676b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    # Moving log file from DBFS to ADLS\n",
    "    dbutils.fs.mv(f\"file:/dbfs/{log_file}\", log_target_dir)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_dims_main",
   "widgets": {
    "file_location": {
     "currentValue": "abfss://olist-ecomm-lakehouse@adlsecommlakehouse.dfs.core.windows.net/Metadata/Gold/Dims/gold_dims_metadata.json",
     "nuid": "bd206a71-b0d8-42e3-952a-0a444a75b050",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "file_location",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "job_name": {
     "currentValue": "Olist_Gold_Dims_Job",
     "nuid": "acea3768-e801-4ff7-88b8-3644802ab2fa",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "job_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "log_target_dir": {
     "currentValue": "abfss://olist-ecomm-lakehouse@adlsecommlakehouse.dfs.core.windows.net/Logs/Gold/Dims",
     "nuid": "1db253c5-8298-4053-9658-0896ff6896d2",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "log_target_dir",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
