{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54f8b57-2620-40de-84d1-32180b9c0a43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **1. Importing required pyspark and user defined modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4f1bb5-81f7-40aa-a99b-b19dc806b2b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing AnalysisException for handling exceptions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7140cf-2324-4d96-a34d-bbc1f5c1ea9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./custom_logging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1a66f8-e313-4115-a216-b79c0de846ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9852923-16e8-486d-963a-f70faf3c3b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./spark_utils\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05cc6f1-20a4-4a31-a103-cf9d0e99353b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **2. Creating widgets to receive the parameters from ADF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29d1607-5dc0-4a21-ab7a-5dd68cd0cea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating widgets to receive the parameters from ADF\n",
    "dbutils.widgets.text(\"job_name\", \"\")\n",
    "dbutils.widgets.text(\"file_location\", \"\")\n",
    "dbutils.widgets.text(\"log_target_dir\", \"\")\n",
    "\n",
    "job_name = dbutils.widgets.get(\"job_name\")\n",
    "file_location = dbutils.widgets.get(\"file_location\")\n",
    "log_target_dir = dbutils.widgets.get(\"log_target_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d241cb-7006-480f-a0cf-d0e588711f41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **3. Configuring the custom logger from custom_logging module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b597b03b-b652-46d1-ba59-e32eb97e90e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating object for the CustomLogger class\n",
    "cust_log = CustomLogger()\n",
    "logger, log_file = cust_log.custom_logger(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99674be-94ac-4064-be51-951a5968d001",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **4. Staring pipeline log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf6557a-6d76-47f7-93b5-1f0eb27d812e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline log\n",
    "logger.info(f\"JOB_NAME : {job_name}\")\n",
    "logger.info(f\"The {job_name} for gold facts delta data load is started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "869fc76e-601e-42e7-a58d-7a93aa0c30ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **5.  Establish SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce4cdf5-fbbb-42b8-b03a-289cd671c1dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Establishing sparksession with service principal authentication configs.\")\n",
    "\n",
    "try:\n",
    "    spark = (\n",
    "            SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Olist_Gold_Facts\") \\\n",
    "            .config(\"fs.azure.account.auth.type.<storage-account-name>.dfs.core.windows.net\", \"OAuth\") \\\n",
    "            .config(\"fs.azure.account.oauth.provider.type.<storage-account-name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n",
    "            .config(\"fs.azure.account.oauth2.client.id.<storage-account-name>.dfs.core.windows.net\", dbutils.secrets.get(scope=\"<secret-scope-name>\", key=\"sp-client-id\")) \\\n",
    "            .config(\"fs.azure.account.oauth2.client.secret.<storage-account-name>.dfs.core.windows.net\", dbutils.secrets.get(scope=\"<secret-scope-name>\", key=\"sp-client-pwd\")) \\\n",
    "            .config(\"fs.azure.account.oauth2.client.endpoint.<storage-account-name>.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope='<secret-scope-name>', key='sp-directory-id')}/oauth2/token\") \\\n",
    "            .getOrCreate()\n",
    "        )\n",
    "except Exception as e:\n",
    "    # Logging Exception details\n",
    "    logger.critical(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "logger.info(\"Sparksession with service principal authentication configs established successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01cf3504-019a-4fba-826a-5fcd978669d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **6. Main function definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d85177b-e6ae-41d1-bd7f-8cb439a256e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    logger.info(f\"In the {__name__} function.\")\n",
    "\n",
    "    # Creating metadata variable from the user defined modules\n",
    "    logger.info(\"Creating metadata variable from the user defined modules.\")\n",
    "\n",
    "    meta_data = ReadJson(file_location)\n",
    "    gold_facts_meta = meta_data.get_metadata()\n",
    "\n",
    "    logger.info(\"Variable created successfully.\")\n",
    "\n",
    "\n",
    "    # Starting the Gold facts delta table data load\n",
    "    logger.info(\"Starting data load for the fact delta tables in Gold layer.\")\n",
    "\n",
    "    # Looping through the gold facts metadata variable 'gold_facts_meta' for performing the data load\n",
    "    for item in gold_facts_meta:\n",
    "\n",
    "        # Pipeline variables from gold_facts_meta metadata variable\n",
    "        fact = item[\"fact\"]\n",
    "        silver_src_tables = item[\"silver_src_tables\"]\n",
    "        dim_src_tables = item[\"dim_src_tables\"]\n",
    "        tgt_table_name = item[\"tgt_table_name\"]\n",
    "        basePath = item[\"basePath\"]\n",
    "\n",
    "        logger.info(f\"Data load started for fact table '{tgt_table_name}'.\")\n",
    "\n",
    "        logger.info(f\"\"\"\n",
    "                        fact      :   {fact}\n",
    "                        silver_src_tables   :   {silver_src_tables}\n",
    "                        dim_src_tables   :   {dim_src_tables}\n",
    "                        tgt_table_name  :   {tgt_table_name}\n",
    "                        basePath    :   {basePath}     \n",
    "                        \"\"\")\n",
    "\n",
    "\n",
    "        try: \n",
    "            # Calling the factory method from spark_utils module to instantiate the object based on the fact\n",
    "            logger.info(f\"Calling the factory method from spark_utils module to instantiate the object based on the fact '{fact}'.\")            \n",
    "            factory = Factory.get_gold_facts(fact)\n",
    "            logger.info(f\"Object instantiated successfully based on the fact name '{fact}'.\")                        \n",
    "\n",
    "\n",
    "            # Creating dictionary of pyspark dataframes from the source Delta tables in Silver & Gold layer using silver_src_tables & dim_src_tables \n",
    "            logger.info(\"Creating dictionary of pyspark dataframes from the source Delta tables in Silver & Gold layer using silver_src_tables & dim_src_tables.\")\n",
    "            silver_df_dict, dims_df_dict = factory.get_src_df_dict(spark, silver_src_tables, dim_src_tables)\n",
    "            logger.info(\"Dictionary of pyspark dataframes has been created successfully.\")\n",
    "\n",
    "            \n",
    "            # Performing the necessary tranformation for the fact table with silver_df_dict, dims_df_dict dictionary of pyspark dataframes\n",
    "            logger.info(\"Performing the necessary tranformation for the fact table with silver_df_dict, dims_df_dict dictionary of pyspark dataframes.\")\n",
    "            src_df = factory.transform_src_df(silver_df_dict, dims_df_dict)\n",
    "            logger.info(\"Tranformation for the fact table has been done successfully.\")        \n",
    "\n",
    "\n",
    "            # Performing upsert data load to the fact delta table in Gold layer using merge\n",
    "            logger.info(f\"Performing upsert data load using merge to the fact delta table '{tgt_table_name}' in Gold layer.\")\n",
    "            factory.load_fact(spark, basePath, src_df)\n",
    "            logger.info(f\"Data loaded successfully to the Gold fact delta table '{tgt_table_name}'.\")\n",
    "\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            # Logging AnalysisException details\n",
    "            logger.error(f\"AnalysisException Message : {str(e)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Logging Exception details\n",
    "            logger.critical(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "    # Shutting down the logger\n",
    "    logger.info(\"Data load for the Gold fact delta tables has been completed.\")\n",
    "    logging.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e77f5a-5c19-4947-86a4-e2b2ab9dafae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **7. Executing the main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f5af71-ce75-4d0f-b919-07b915676b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    # Moving log file from DBFS to ADLS\n",
    "    dbutils.fs.mv(f\"file:/dbfs/{log_file}\", log_target_dir)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_facts_main",
   "widgets": {
    "file_location": {
     "currentValue": "abfss://olist-ecomm-lakehouse@adlsecommlakehouse.dfs.core.windows.net/Metadata/Gold/Facts/gold_facts_metadata.json",
     "nuid": "4d4b08b3-bf53-4b9d-ade4-6b81be9eb24d",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "file_location",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "job_name": {
     "currentValue": "Olist_Gold_Facts_Job",
     "nuid": "e194f8eb-9824-4689-9b14-379ac80f9248",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "job_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "log_target_dir": {
     "currentValue": "abfss://olist-ecomm-lakehouse@adlsecommlakehouse.dfs.core.windows.net/Logs/Gold/Facts",
     "nuid": "1c33cf5e-7d35-4bf4-833b-2986d695bdbb",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "log_target_dir",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
