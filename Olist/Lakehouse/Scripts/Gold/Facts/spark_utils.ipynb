{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897a46eb-8bd3-43d2-a8bc-4a1df06864f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **1. Importing the required modules and functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4613a2b4-ee1a-42e7-9aba-bf9119978620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required modules and functions\n",
    "from pyspark.sql.functions import col, date_trunc, to_date, to_timestamp, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "529c9871-37a3-4424-a854-888f55a428b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **2. Abstract base class for factory interface.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b000c9-4efa-4cb2-be7a-1fc707741193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating abstract base class for factory interface\n",
    "class Gold_Facts(ABC):\n",
    "    # Basic representation of the data extraction & loading codes\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_src_df_dict(self, spark, silver_src_tables, dim_src_tables):\n",
    "        # Read and returns a dictionary of pyspark dataframes from the source Delta tables in Silver & Gold layer\n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform_src_df(self, silver_df_dict, dims_df_dict):\n",
    "        # Computes the necessary tranformation for the fact table with src_df_dict dictionary of pyspark dataframes\n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")        \n",
    "\n",
    "    @abstractmethod\n",
    "    def load_fact(self, spark, basePath, src_df):\n",
    "        # Performs data load for Fact Delta tables in Gold layer with source pyspark dataframe \n",
    "        raise NotImplementedError(\"This method must be overridden by subclasses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c6c15f-24e9-4507-9240-0b042311886e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **3. Concrete classes and implementing the abstract methods in subclasses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366b9a01-033c-48ed-9183-4bf63f043921",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Sales(Gold_Facts):\n",
    "\n",
    "    # Read and returns a dictionaries of pyspark dataframes from the source Delta tables in Silver & Gold layer\n",
    "    def get_src_df_dict(self, spark, silver_src_tables, dim_src_tables):\n",
    "\n",
    "        # Creates a dictionary of pyspark dataframes from the source Delta tables in Silver layer using dictionary comprehension\n",
    "        silver_df_dict = {key : DeltaTable.forPath(spark, value).toDF() for (key, value) in silver_src_tables.items()}\n",
    "\n",
    "        # Creates a dictionary of pyspark dataframes from the source Delta tables in Gold layer using dictionary comprehension\n",
    "        dims_df_dict = {key : DeltaTable.forPath(spark, value).toDF().filter(col(\"is_current\") == 1) for (key, value) in dim_src_tables.items()} \n",
    "\n",
    "        return silver_df_dict, dims_df_dict\n",
    "    \n",
    "\n",
    "    # Computes the necessary tranformation for the fact table with silver_df_dict, dims_df_dict dictionary of pyspark dataframes\n",
    "    def transform_src_df(self, silver_df_dict, dims_df_dict):\n",
    "\n",
    "        # Creates the sales pyspark dataframe using the dataframe dictionary\n",
    "        sales_df = silver_df_dict[\"dim_olist_date\"].alias(\"dt\") \\\n",
    "            .join(dims_df_dict[\"dim_olist_orders\"].alias(\"o\"), (to_date(col(\"dt.date_skey\"), \"yyyyMMdd\") == date_trunc(\"day\", col(\"o.order_purchase_timestamp\").cast(\"date\")))) \\\n",
    "            .join(dims_df_dict[\"dim_olist_customers\"].alias(\"c\"), (col(\"o.customer_id\") == col(\"c.customer_id\"))) \\\n",
    "            .join(silver_df_dict[\"silver_olist_order_items\"].alias(\"oi\"), (col(\"o.order_id\") == col(\"oi.order_id\"))) \\\n",
    "            .join(dims_df_dict[\"dim_olist_products\"].alias(\"p\"), (col(\"oi.product_id\") == col(\"p.product_id\"))) \\\n",
    "            .join(dims_df_dict[\"dim_olist_sellers\"].alias(\"s\"), (col(\"p.seller_id\") == col(\"s.seller_id\"))) \\\n",
    "            .where(col(\"o.order_status\") == \"Delivered\")\n",
    "\n",
    "        sales_df = sales_df.select(\n",
    "            col(\"dt.date_skey\").alias(\"date_skey\"),\n",
    "            col(\"c.customer_skey\").alias(\"customer_skey\"),\n",
    "            col(\"o.order_skey\").alias(\"order_skey\"),\n",
    "            col(\"p.product_skey\").alias(\"product_skey\"),\n",
    "            col(\"s.seller_skey\").alias(\"seller_skey\"),\n",
    "            to_timestamp(col(\"o.order_purchase_timestamp\"), \"yyyy-MM-dd hh:mi:ss\").alias(\"order_purchase_timestamp\"),\n",
    "            col(\"dt.cal_month_name\").alias(\"order_purchase_month\"),\n",
    "            col(\"p.product_category_name_english\").alias(\"product_category_name_english\"),\n",
    "            col(\"oi.item_quantity\").cast(\"int\").alias(\"sales_quantity\"),\n",
    "            col(\"p.price\").cast(\"float\").alias(\"price_per_unit\"),\n",
    "            col(\"p.freight_value\").cast(\"float\").alias(\"freight_value_per_unit\"),\n",
    "            ((col(\"p.price\") * col(\"oi.item_quantity\")) + ((col(\"p.freight_value\") * col(\"oi.item_quantity\")))).alias(\"net_sales_amount\"),\n",
    "            col(\"c.customer_state\").alias(\"customer_state\"),\n",
    "            col(\"c.customer_state_code\").alias(\"customer_state_code\")\n",
    "            )\n",
    "        \n",
    "        return sales_df\n",
    "\n",
    "\n",
    "    # Performs upsert to the Sales fact delta table in Gold layer \n",
    "    def load_fact(self, spark, basePath, src_df):\n",
    "\n",
    "        # Reading sales fact delta table from gold layer\n",
    "        fact_olist_sales = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Apply upsert operation using merge\n",
    "        fact_olist_sales.alias(\"tgt\").merge(\n",
    "        src_df.alias(\"sales_df\"),\n",
    "        \"\"\"((\n",
    "            (sales_df.date_skey = tgt.date_skey) AND\n",
    "            (sales_df.customer_skey = tgt.customer_skey) AND\n",
    "            (sales_df.order_skey = tgt.order_skey) AND\n",
    "            (sales_df.product_skey = tgt.product_skey) AND\n",
    "            (sales_df.seller_skey = tgt.seller_skey)\n",
    "        ))\"\"\") \\\n",
    "        .whenMatchedUpdate(\n",
    "        set = {\n",
    "            \"order_purchase_timestamp\": \"sales_df.order_purchase_timestamp\",\n",
    "            \"order_purchase_month\": \"sales_df.order_purchase_month\",\n",
    "            \"product_category_name_english\": \"sales_df.product_category_name_english\",\n",
    "            \"sales_quantity\": \"sales_df.sales_quantity\",\n",
    "            \"price_per_unit\": \"sales_df.price_per_unit\",  \n",
    "            \"freight_value_per_unit\": \"sales_df.freight_value_per_unit\",          \n",
    "            \"net_sales_amount\": \"sales_df.net_sales_amount\",\n",
    "            \"customer_state\": \"sales_df.customer_state\",\n",
    "            \"customer_state_code\": \"sales_df.customer_state_code\",\n",
    "            \"load_date\": current_timestamp()\n",
    "        }\n",
    "        ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"date_skey\": \"sales_df.date_skey\",            \n",
    "            \"customer_skey\": \"sales_df.customer_skey\",\n",
    "            \"order_skey\": \"sales_df.order_skey\",\n",
    "            \"product_skey\": \"sales_df.product_skey\",\n",
    "            \"seller_skey\": \"sales_df.seller_skey\",                                    \n",
    "            \"order_purchase_timestamp\": \"sales_df.order_purchase_timestamp\",\n",
    "            \"order_purchase_month\": \"sales_df.order_purchase_month\",\n",
    "            \"product_category_name_english\": \"sales_df.product_category_name_english\",\n",
    "            \"sales_quantity\": \"sales_df.sales_quantity\",\n",
    "            \"price_per_unit\": \"sales_df.price_per_unit\",  \n",
    "            \"freight_value_per_unit\": \"sales_df.freight_value_per_unit\",          \n",
    "            \"net_sales_amount\": \"sales_df.net_sales_amount\",\n",
    "            \"customer_state\": \"sales_df.customer_state\",\n",
    "            \"customer_state_code\": \"sales_df.customer_state_code\",\n",
    "            \"load_date\": current_timestamp()\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0fb63e5-25a4-414f-97e2-bbab458e337c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Payments(Gold_Facts):\n",
    "\n",
    "    # Read and returns a dictionaries of pyspark dataframes from the source Delta tables in Silver & Gold layer\n",
    "    def get_src_df_dict(self, spark, silver_src_tables, dim_src_tables):\n",
    "\n",
    "        # Creates a dictionary of pyspark dataframes from the source Delta tables in Silver layer using dictionary comprehension\n",
    "        silver_df_dict = {key : DeltaTable.forPath(spark, value).toDF() for (key, value) in silver_src_tables.items()}\n",
    "\n",
    "        # Creates a dictionary of pyspark dataframes from the source Delta tables in Gold layer using dictionary comprehension\n",
    "        dims_df_dict = {key : DeltaTable.forPath(spark, value).toDF().filter(col(\"is_current\") == 1) for (key, value) in dim_src_tables.items()} \n",
    "\n",
    "        return silver_df_dict, dims_df_dict\n",
    "    \n",
    "\n",
    "    # Computes the necessary tranformation for the fact table with silver_df_dict, dims_df_dict dictionary of pyspark dataframes\n",
    "    def transform_src_df(self, silver_df_dict, dims_df_dict):\n",
    "\n",
    "        # Creates the payments pyspark dataframe using the dataframe dictionary\n",
    "        payments_df = silver_df_dict[\"dim_olist_date\"].alias(\"dt\") \\\n",
    "            .join(dims_df_dict[\"dim_olist_orders\"].alias(\"o\"), (to_date(col(\"dt.date_skey\"), \"yyyyMMdd\") == date_trunc(\"day\", col(\"o.order_approved_at\").cast(\"date\")))) \\\n",
    "            .join(dims_df_dict[\"dim_olist_customers\"].alias(\"c\"), (col(\"o.customer_id\") == col(\"c.customer_id\"))) \\\n",
    "            .join(silver_df_dict[\"silver_olist_order_payments\"].alias(\"op\"), (col(\"o.order_id\") == col(\"op.order_id\")))\n",
    "\n",
    "\n",
    "        payments_df = payments_df.select(\n",
    "            col(\"dt.date_skey\").alias(\"date_skey\"),\n",
    "            col(\"c.customer_skey\").alias(\"customer_skey\"),\n",
    "            col(\"o.order_skey\").alias(\"order_skey\"),\n",
    "            col(\"o.order_approved_at\").alias(\"payment_approved_at\"),\n",
    "            col(\"dt.cal_month_name\").alias(\"order_purchase_month\"),\n",
    "            col(\"op.payment_sequential\").alias(\"payment_sequential\"),\n",
    "            col(\"op.payment_type\").alias(\"payment_type\"),\n",
    "            col(\"op.payment_value\").alias(\"payment_value\"),\n",
    "            col(\"o.order_status\").alias(\"order_status\")\n",
    "            )\n",
    "        \n",
    "        return payments_df\n",
    "\n",
    "\n",
    "    # Performs upsert to the Payments fact delta table in Gold layer \n",
    "    def load_fact(self, spark, basePath, src_df):\n",
    "\n",
    "        # Reading payments fact delta table from gold layer\n",
    "        fact_olist_order_payments = DeltaTable.forPath(spark, f\"abfss://{basePath}\")\n",
    "\n",
    "        # Apply upsert operation using merge\n",
    "        fact_olist_order_payments.alias(\"tgt\").merge(\n",
    "        src_df.alias(\"payments_df\"),\n",
    "        \"\"\"((\n",
    "            (payments_df.date_skey = tgt.date_skey) AND\n",
    "            (payments_df.customer_skey = tgt.customer_skey) AND\n",
    "            (payments_df.order_skey = tgt.order_skey) AND\n",
    "            (payments_df.payment_sequential = tgt.payment_sequential)\n",
    "        ))\"\"\") \\\n",
    "        .whenMatchedUpdate(\n",
    "        set = {\n",
    "            \"payment_approved_at\": \"payments_df.payment_approved_at\",\n",
    "            \"order_purchase_month\": \"payments_df.order_purchase_month\",\n",
    "            \"payment_type\": \"payments_df.payment_type\",\n",
    "            \"payment_value\": \"payments_df.payment_value\",\n",
    "            \"order_status\": \"payments_df.order_status\",\n",
    "            \"load_date\": current_timestamp()\n",
    "        }\n",
    "        ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"date_skey\": \"payments_df.date_skey\",            \n",
    "            \"customer_skey\": \"payments_df.customer_skey\",\n",
    "            \"order_skey\": \"payments_df.order_skey\",\n",
    "            \"payment_approved_at\": \"payments_df.payment_approved_at\",\n",
    "            \"order_purchase_month\": \"payments_df.order_purchase_month\",\n",
    "            \"payment_sequential\": \"payments_df.payment_sequential\",\n",
    "            \"payment_type\": \"payments_df.payment_type\",\n",
    "            \"payment_value\": \"payments_df.payment_value\",  \n",
    "            \"order_status\": \"payments_df.order_status\",          \n",
    "            \"load_date\": current_timestamp()\n",
    "        }\n",
    "        ).execute()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052bdcc9-f996-45d4-a7b8-fc97c330ca44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **4. Factory class with static method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fae9a9d-2c2f-4d75-ab0e-2de75fdff913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Factory class with static method\n",
    "class Factory:\n",
    "\n",
    "    # Method to return the instance based on the target fact table\n",
    "    @staticmethod\n",
    "    def get_gold_facts(fact: str) -> Gold_Facts:\n",
    "\n",
    "        facts = {\n",
    "        \"Sales\" : Sales(),\n",
    "        \"Payments\" : Payments()\n",
    "        }\n",
    "\n",
    "        while True:\n",
    "            # If fact is present in the facts dictionary it'll return the object of the concrete class\n",
    "            if fact in facts:\n",
    "                return facts[fact]\n",
    "            \n",
    "            print(f\"Unknown fact : {fact}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
