{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54f8b57-2620-40de-84d1-32180b9c0a43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **1. Importing required pyspark and user defined modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cf7481-73ce-4de5-a85d-2716d3c39c9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing AnalysisException for handling exceptions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7140cf-2324-4d96-a34d-bbc1f5c1ea9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./custom_logging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1a66f8-e313-4115-a216-b79c0de846ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9852923-16e8-486d-963a-f70faf3c3b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./spark_utils\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05cc6f1-20a4-4a31-a103-cf9d0e99353b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **2. Creating widgets to receive the parameters from ADF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29d1607-5dc0-4a21-ab7a-5dd68cd0cea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating widgets to receive the parameters from ADF\n",
    "dbutils.widgets.text(\"job_name\", \"\")\n",
    "dbutils.widgets.text(\"file_location\", \"\")\n",
    "dbutils.widgets.text(\"log_target_dir\", \"\")\n",
    "\n",
    "job_name = dbutils.widgets.get(\"job_name\")\n",
    "file_location = dbutils.widgets.get(\"file_location\")\n",
    "log_target_dir = dbutils.widgets.get(\"log_target_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d241cb-7006-480f-a0cf-d0e588711f41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **3. Configuring the custom logger from custom_logging module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b597b03b-b652-46d1-ba59-e32eb97e90e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating object for the CustomLogger class\n",
    "cust_log = CustomLogger()\n",
    "logger, log_file = cust_log.custom_logger(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99674be-94ac-4064-be51-951a5968d001",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **4. Staring pipeline log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf6557a-6d76-47f7-93b5-1f0eb27d812e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline log\n",
    "logger.info(f\"JOB_NAME : {job_name}\")\n",
    "logger.info(f\"The {job_name} for bronze archive delta data load is started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68a5cb21-8823-42de-928b-8d50de3b68cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **5.  Establish SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4cdf10e-711a-4b8c-83b7-f9204245903f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Establishing sparksession with service principal authentication configs.\")\n",
    "\n",
    "try:\n",
    "    spark = (\n",
    "            SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Olist_Bronze_Archive\") \\\n",
    "            .config(\"fs.azure.account.auth.type.<storage-account-name>.dfs.core.windows.net\", \"OAuth\") \\\n",
    "            .config(\"fs.azure.account.oauth.provider.type.<storage-account-name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\n",
    "            .config(\"fs.azure.account.oauth2.client.id.<storage-account-name>.dfs.core.windows.net\", dbutils.secrets.get(scope=\"<secret-scope-name>\", key=\"sp-client-id\")) \\\n",
    "            .config(\"fs.azure.account.oauth2.client.secret.<storage-account-name>.dfs.core.windows.net\", dbutils.secrets.get(scope=\"<secret-scope-name>\", key=\"sp-client-pwd\")) \\\n",
    "            .config(\"fs.azure.account.oauth2.client.endpoint.<storage-account-name>.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope='<secret-scope-name>', key='sp-directory-id')}/oauth2/token\") \\\n",
    "            .getOrCreate()\n",
    "        )\n",
    "except Exception as e:\n",
    "    # Logging Exception details\n",
    "    logger.critical(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "logger.info(\"Sparksession with service principal authentication configs established successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01cf3504-019a-4fba-826a-5fcd978669d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **6. Main function definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d85177b-e6ae-41d1-bd7f-8cb439a256e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    logger.info(f\"In the {__name__} function.\")\n",
    "\n",
    "    #Creating objects for the user defined modules\n",
    "    logger.info(\"Creating objects for the user defined modules.\")\n",
    "\n",
    "    meta_data = ReadJson(file_location)\n",
    "    spark_utils = SparkUtils()\n",
    "    archive_meta = meta_data.get_metadata()\n",
    "\n",
    "    logger.info(\"Objects created successfully.\")\n",
    "\n",
    "        \n",
    "    # Starting the bronze archive delta table data load\n",
    "    logger.info(\"Starting data load for the Bronze archive delta tables.\")\n",
    "\n",
    "    # Looping through the archive metadata variable 'archive_meta' for performing the data load\n",
    "    for item in archive_meta:\n",
    "\n",
    "        # Pipeline variables from archive_meta metadata variable\n",
    "        app_nm = item[\"app_nm\"]\n",
    "        file_name = item[\"file_name\"]\n",
    "        file_path = item[\"file_path\"]\n",
    "        columns = item[\"columns\"]\n",
    "        table_name = item[\"table_name\"]\n",
    "        basePath = item[\"basePath\"]\n",
    "\n",
    "        logger.info(f\"Data load started for application '{app_nm}'.\")\n",
    "\n",
    "        logger.info(f\"\"\"\n",
    "                        app_nm      :   {app_nm}\n",
    "                        file_name   :   {file_name}\n",
    "                        file_path   :   {file_path}\n",
    "                        columns     :   {columns}\n",
    "                        table_name  :   {table_name}\n",
    "                        basePath    :   {basePath}     \n",
    "                        \"\"\")\n",
    "\n",
    "        try: \n",
    "            # Creating pyspark dataframe from bronze staging table\n",
    "            logger.info(f\"Creating pyspark dataframe from bronze staging parquet file '{file_name}'.\")\n",
    "            df = spark_utils.read_parquet_file(spark, file_path, columns)\n",
    "            logger.info(f\"Dataframe has been created successfully from bronze staging parquet file '{file_name}'.\")\n",
    "\n",
    "\n",
    "            # Adding load_date audit column to the dataframe\n",
    "            logger.info(f\"Adding load_date audit column to the dataframe.\")\n",
    "            bronze_df = spark_utils.add_audit_column(df)\n",
    "            logger.info(f\"Audit column added to the dataframe successfully.\")\n",
    "\n",
    "\n",
    "            # Deleting 1 year older and current day records if any from the bronze archive delta table.\n",
    "            logger.info(f\"Deleting 1 year older records and current day records if any from the '{table_name}' table.\")\n",
    "            spark_utils.del_archive_records(spark, basePath)\n",
    "            logger.info(f\"Deleted 1 year older records and current day records if any from the '{table_name}' table.\")\n",
    "\n",
    "\n",
    "            # Loading data to the bronze archive delta table\n",
    "            logger.info(f\"Loading data to the bronze archive delta table '{table_name}'.\")\n",
    "            spark_utils.load_archive_table(bronze_df, basePath)\n",
    "            logger.info(f\"Data loaded successfully to the bronze archive delta table '{table_name}'.\")\n",
    "\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            # Logging AnalysisException details\n",
    "            logger.error(f\"AnalysisException Message : {str(e)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Logging Exception details\n",
    "            logger.critical(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "    # Shutting down the logger\n",
    "    logger.info(\"Data load for the Bronze archive delta tables has been completed.\")\n",
    "    logging.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e77f5a-5c19-4947-86a4-e2b2ab9dafae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **6. Executing the main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f5af71-ce75-4d0f-b919-07b915676b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    main()\\n    \\n    # Moving log file from DBFS to ADLS\\n    dbutils.fs.mv(f\"file:/dbfs/{log_file}\", log_target_dir)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    # Moving log file from DBFS to ADLS\n",
    "    dbutils.fs.mv(f\"file:/dbfs/{log_file}\", log_target_dir)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "archive_main",
   "widgets": {
    "file_location": {
     "currentValue": "",
     "nuid": "371778dc-d26c-4b58-be8b-c308fc6b8f86",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "file_location",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "job_name": {
     "currentValue": "",
     "nuid": "f8c719d8-faf4-429b-acfc-d8619edd36e7",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "job_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "log_target_dir": {
     "currentValue": "",
     "nuid": "ed545f81-d10a-41f1-bac2-f10c1c6c1c90",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "log_target_dir",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
