{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4613a2b4-ee1a-42e7-9aba-bf9119978620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required modules and functions\n",
    "from pyspark.sql.functions import col, current_date, current_timestamp\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4975d593-0b99-48ee-b8cb-456655e548f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SparkUtils:\n",
    "    \n",
    "    # Method to read the parquet files from bronze staging layer with read options & return the pyspark dataframe    \n",
    "    def read_parquet_file(self, spark, file_path, columns):\n",
    "        \n",
    "        # Reading the parquet file using spark & function args\n",
    "        df = (\n",
    "            spark \\\n",
    "            .read \\\n",
    "            .parquet(f\"abfss://{file_path}\")\n",
    "        )\n",
    "        \n",
    "        # Fetching specified columns from the dataframe using columns list arg        \n",
    "        df = (\n",
    "            df \\\n",
    "            .select(*columns)\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    # Method to add load_date audit column to the pyspark dataframe returned by read_parquet_file method\n",
    "    def add_audit_column(self, df):\n",
    "\n",
    "        # Adding audit column to the pyspark dataframe\n",
    "        df = (\n",
    "            df \\\n",
    "            .withColumn(\"load_date\", current_timestamp())\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    # Method to delete 1 year old and current day records if any, with the date functions\n",
    "    def del_archive_records(self, spark, basePath):\n",
    "\n",
    "        # Read the archive delta table\n",
    "        archive_table = (\n",
    "            DeltaTable \\\n",
    "            .forPath(spark, f\"abfss://{basePath}\")\n",
    "        )\n",
    "\n",
    "        # Deleting the records\n",
    "        archive_table.delete(\"CAST(load_date AS DATE) < date_sub(current_date(), 365) OR CAST(load_date AS DATE) = current_date()\")\n",
    "\n",
    "\n",
    "    # Method to append the Delta archive table with the bronze pyspark dataframe\n",
    "    def load_archive_table(self, df, basePath):\n",
    "        \n",
    "        # Writing data to the table in the specified basepath\n",
    "        df \\\n",
    "        .write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(f\"abfss://{basePath}\")\n",
    "\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
